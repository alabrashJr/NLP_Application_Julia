{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "**Reference:** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" In Advances in neural information processing systems, pp. 3104-3112. 2014. ([Paper](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks), [Sample code](https://github.com/tensorflow/nmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Assignment has been done by\n",
    "\n",
    "<h2> Abdurrahman Beyaz</h2>\n",
    "00684383 \n",
    "\n",
    "[Email](aalabrash18@ku.edu.tr) \n",
    "\n",
    "<h2> Ahmed Masry </h2>\n",
    "0061868\n",
    "\n",
    "[Email](amasry17@ku.edu.tr )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[To Download model s2s_v1.jld2](https://drive.google.com/uc?export=download&confirm=iXgT&id=1ztqkspEia4e3yE9VYfyS5olAwKgH33On) BLEU ~3\n",
    "\n",
    "[To Download model s2s_v1.1.jld2](https://drive.google.com/uc?id=1b9YXxrMeZqHayPhlXyXg5_f-6CKR0RYb&export=download) BLEU ~3\n",
    "\n",
    "[s2s_v2_f_4.jld2]( https://drive.google.com/open?id=1KK25lQ9vbBdavi9nnle2FxF72dlu47yC) BLEU ~8\n",
    "\n",
    "[s2s_v2.jld](https://drive.google.com/file/d/1Sj8IAMvgCApB85UC4w46SibBTsrQTZ-S/view?usp=sharing) hightest BLEU 8.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@size (macro with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Test, Base.Iterators, IterTools, Random # , LinearAlgebra, StatsBase\n",
    "using AutoGrad: @gcheck  # to check gradients, use with Float64\n",
    "Knet.atype() = KnetArray{Float32}  # determines what Knet.param() uses.\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part -1. Types from the last project\n",
    "\n",
    "Please copy the following types and related functions from the last project: `Vocab`,\n",
    "`TextReader`, `Embed`, `Linear`, `mask!`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end\n",
    "\n",
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    word_count = Dict{String,Int}()\n",
    "    w2i = Dict{String,Int}()\n",
    "    i2w = Vector{String}()\n",
    "    int_unk = get!(w2i, unk, 1+length(w2i))\n",
    "    int_eos = get!(w2i, eos, 1+length(w2i))\n",
    "    for line in eachline(file)\n",
    "        line = tokenizer(line)\n",
    "        for word in line\n",
    "            if haskey(word_count, word)\n",
    "                word_count[word] += 1\n",
    "            else\n",
    "                word_count[word] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    word_count = collect(word_count)\n",
    "    sort!(word_count, rev=true, by=x->x[2])\n",
    "    # constructing w2i\n",
    "    for pair in word_count\n",
    "        if pair[2] >= mincount\n",
    "            get!(w2i, pair[1], 1+length(w2i))\n",
    "            if length(w2i) >= vocabsize\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    w2i_array = collect(w2i)\n",
    "    sort!(w2i_array, by=x->x[2])\n",
    "    for pair in w2i_array\n",
    "        push!(i2w, pair[1])\n",
    "    end\n",
    "    return Vocab(w2i, i2w, int_unk, int_eos, tokenizer)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mask! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end\n",
    "\n",
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    # Your code here\n",
    "    s ==nothing ? file = open(r.file) : file =s\n",
    "    \n",
    "    if eof(file) == true\n",
    "        close(file)\n",
    "        return nothing\n",
    "    end\n",
    "    line = readline(file)\n",
    "    text = r.vocab.tokenizer(line)\n",
    "    arr = [get(r.vocab.w2i,word,r.vocab.w2i[\"<unk>\"]) for word in text ]\n",
    "    return (arr, file)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}\n",
    "\n",
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    # Your code here\n",
    "    Embed(param(embedsize, vocabsize, atype = KnetArray{Float32}))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    # Your code here\n",
    "    l.w[:,x]\n",
    "end\n",
    "\n",
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    # Your code here\n",
    "    Linear(param(outputsize, inputsize, atype = KnetArray{Float32}), param0(outputsize, atype = KnetArray{Float32}))\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    # Your code here\n",
    "    l.w*x .+ l.b\n",
    "end\n",
    "\n",
    "function mask!(a,pad)\n",
    "    # Your code here\n",
    "    #b = deepcopy(a)\n",
    "    for k in 1:size(a, 1)\n",
    "        if a[k , size(a, 2)]!= pad\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        indices = []\n",
    "        for i in 1:size(a[k, :], 1)\n",
    "            if a[k, i] == pad\n",
    "                push!(indices, i)\n",
    "            end\n",
    "        end\n",
    "        indices = reverse(indices)\n",
    "        for j in 1:size(indices, 1)-1\n",
    "            if indices[j] == indices[j+1] + 1\n",
    "                a[k, indices[j]] = 0\n",
    "            else\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    a\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Load data\n",
    "\n",
    "We will use the Turkish-English pair from the [TED Talks Dataset](https://github.com/neulab/word-embeddings-for-nmt) for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing data\n",
      "└ @ Main In[4]:18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datadir = \"datasets/tr_to_en\"\n",
    "\n",
    "if !isdir(datadir)\n",
    "    download(\"http://www.phontron.com/data/qi18naacl-dataset.tar.gz\", \"qi18naacl-dataset.tar.gz\")\n",
    "    run(`tar xzf qi18naacl-dataset.tar.gz`)\n",
    "end\n",
    "\n",
    "if !isdefined(Main, :tr_vocab)\n",
    "    tr_vocab = Vocab(\"$datadir/tr.train\", mincount=5)\n",
    "    en_vocab = Vocab(\"$datadir/en.train\", mincount=5)\n",
    "    tr_train = TextReader(\"$datadir/tr.train\", tr_vocab)\n",
    "    en_train = TextReader(\"$datadir/en.train\", en_vocab)\n",
    "    tr_dev = TextReader(\"$datadir/tr.dev\", tr_vocab)\n",
    "    en_dev = TextReader(\"$datadir/en.dev\", en_vocab)\n",
    "    tr_test = TextReader(\"$datadir/tr.test\", tr_vocab)\n",
    "    en_test = TextReader(\"$datadir/en.test\", en_vocab)\n",
    "    @info \"Testing data\"\n",
    "    @test length(tr_vocab.i2w) == 38126\n",
    "    @test length(first(tr_test)) == 16\n",
    "    @test length(collect(tr_test)) == 5029\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Minibatching\n",
    "\n",
    "For minibatching we are going to design a new iterator: `MTData`. This iterator is built\n",
    "on top of two TextReaders `src` and `tgt` that produce parallel sentences for source and\n",
    "target languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MTData\n",
    "    src::TextReader        # reader for source language data\n",
    "    tgt::TextReader        # reader for target language data\n",
    "    batchsize::Int         # desired batch size\n",
    "    maxlength::Int         # skip if source sentence above maxlength\n",
    "    batchmajor::Bool       # batch dims (B,T) if batchmajor=false (default) or (T,B) if true.\n",
    "    bucketwidth::Int       # batch sentences with length within bucketwidth of each other\n",
    "    buckets::Vector        # sentences collected in separate arrays called buckets for each length range\n",
    "    batchmaker::Function   # function that turns a bucket into a batch.\n",
    "end\n",
    "\n",
    "function MTData(src::TextReader, tgt::TextReader; batchmaker = arraybatch, batchsize = 128, maxlength = typemax(Int),\n",
    "                batchmajor = false, bucketwidth = 10, numbuckets = min(128, maxlength ÷ bucketwidth))\n",
    "    buckets = [ [] for i in 1:numbuckets ] # buckets[i] is an array of sentence pairs with similar length\n",
    "    MTData(src, tgt, batchsize, maxlength, batchmajor, bucketwidth, buckets, batchmaker)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{MTData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{MTData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{MTData}) = NTuple{2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate(::MTData)\n",
    "\n",
    "Define the `iterate` function for the `MTData` iterator. `iterate` should return a\n",
    "`(batch, state)` pair or `nothing` if there are no more batches.  The `batch` is a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. The `state` is a pair of `(src_state,tgt_state)` which can be used\n",
    "to iterate `d.src` and `d.tgt` to get more sentences.  `iterate(d)` without a second\n",
    "argument should initialize `d` by emptying its buckets and calling `iterate` on the inner\n",
    "iterators `d.src` and `d.tgt` without a state. Please review the documentation on\n",
    "iterators from the last project.\n",
    "\n",
    "To keep similar length sentences together `MTData` uses arrays of similar length sentence\n",
    "pairs called buckets.  Specifically, the `(src_sentence,tgt_sentence)` pairs coming from\n",
    "`src` and `tgt` are pushed into `d.buckets[i]` when the length of the source sentence is\n",
    "in the range `((i-1)*d.bucketwidth+1):(i*d.bucketwidth)`. When one of the buckets reaches\n",
    "`d.batchsize` `d.batchmaker` is called with the full bucket producing a 2-D batch, the\n",
    "bucket is emptied and the batch is returned. If `src` and `tgt` are exhausted the\n",
    "remaining partially full buckets are turned into batches and returned in any order. If the\n",
    "source sentence length is larger than `length(d.buckets)*d.bucketwidth`, the last bucket\n",
    "is used.\n",
    "\n",
    "Sentences above a certain length can be skipped using the `d.maxlength` field, and\n",
    "transposed `x,y` arrays can be produced using the `d.batchmajor` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(d::MTData, state=nothing)\n",
    "    # Your code here\n",
    "    \n",
    "    batch = nothing\n",
    "    state = state\n",
    "    if state == nothing\n",
    "        for i in 1:length(d.buckets)\n",
    "           d.buckets[i] = [] \n",
    "        end\n",
    "    end\n",
    "    \n",
    "    while batch == nothing\n",
    "        #print(\"W\")\n",
    "        if state == \"index\"\n",
    "            #println(\"Hello. It's me.\")\n",
    "            found = false\n",
    "            for i in 1:length(d.buckets)\n",
    "                buck = d.buckets[i]\n",
    "                if length(buck) != 0\n",
    "                    #println(length(buck))\n",
    "                    found = true\n",
    "                    batch = d.batchmaker(d, buck) \n",
    "                    d.buckets[i] = []\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "            \n",
    "            if found\n",
    "                return (batch, \"index\") \n",
    "            else\n",
    "                #print(\"Nothing\")\n",
    "                return nothing\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        state_src, state_tgt = state==nothing ? (nothing, nothing) : state\n",
    "        src_pair = iterate(d.src, state_src)\n",
    "        tgt_pair = iterate(d.tgt, state_tgt)\n",
    "        \n",
    "        if src_pair == nothing\n",
    "            #println(\"Index reached\")\n",
    "            state = \"index\"\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        src_sentence, state_src = src_pair\n",
    "        tgt_sentence, state_tgt = tgt_pair\n",
    "        \n",
    "        if length(src_sentence) > d.maxlength\n",
    "            state = (state_src, state_tgt)\n",
    "            continue\n",
    "        end\n",
    "        if length(src_sentence) > length(d.buckets)*d.bucketwidth\n",
    "            last_bucket = d.buckets[length(d.buckets)]\n",
    "            push!(last_bucket, (src_sentence, tgt_sentence))\n",
    "            if length(last_bucket) == d.batch_size\n",
    "               batch = d.batchmaker(d, last_bucket)\n",
    "               d.buckets[length(d.buckets)] = []\n",
    "            end\n",
    "        else \n",
    "            for i in 1:length(d.buckets)\n",
    "               if (length(src_sentence) >= ((i-1)*d.bucketwidth+1)) & (length(src_sentence) <= (i*d.bucketwidth))\n",
    "                   push!(d.buckets[i], (src_sentence, tgt_sentence))\n",
    "                    if length(d.buckets[i]) == d.batchsize\n",
    "                        #println(\"I am here\")\n",
    "                        batch = d.batchmaker(d, d.buckets[i])\n",
    "                        d.buckets[i] = []\n",
    "                    end\n",
    "                    break\n",
    "                end\n",
    "            end   \n",
    "        end\n",
    "        state = (state_src, state_tgt)\n",
    "    end\n",
    "    #print(\"Bye\")\n",
    "    return (batch, state)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arraybatch\n",
    "\n",
    "Define `arraybatch(d, bucket)` to be used as the default `d.batchmaker`. `arraybatch`\n",
    "takes an `MTData` object and an array of sentence pairs `bucket` and returns a\n",
    "`(x::Matrix{Int},y::Matrix{Int})` pair where `x` is a `(batchsize,srclength)` batch of\n",
    "source language sentences and `y` is a `(batchsize,tgtlength)` batch of parallel target\n",
    "language translations. Note that the sentences in the bucket do not have any `eos` tokens\n",
    "and they may have different lengths. `arraybatch` should copy the source sentences into\n",
    "`x` padding shorter ones on the left with `eos` tokens. It should copy the target\n",
    "sentences into `y` with an `eos` token in the beginning and end of each sentence and\n",
    "shorter sentences padded on the right with extra `eos` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arraybatch (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function arraybatch(d::MTData, bucket)\n",
    "    # Your code here\n",
    "    #println(\"Bucket: \", length(bucket), \", \", length(bucket[1][1]), \", \", length(bucket[1][2]))\n",
    "    srclength = 0\n",
    "    tgtlength = 0\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for pair in bucket\n",
    "        src_sentence, tgt_sentence = pair\n",
    "        if length(src_sentence) > srclength\n",
    "           srclength = length(src_sentence)\n",
    "        end\n",
    "        if length(tgt_sentence) > tgtlength\n",
    "           tgtlength = length(tgt_sentence)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    for pair in bucket\n",
    "        src_sentence, tgt_sentence = pair\n",
    "        src_sen = []\n",
    "        tgt_sen = []\n",
    "                \n",
    "        #Target part. \n",
    "        tgt_eos = d.tgt.vocab.eos\n",
    "        push!(tgt_sen, tgt_eos)\n",
    "        for w in tgt_sentence\n",
    "            push!(tgt_sen, w) \n",
    "        end\n",
    "        while length(tgt_sen) != (tgtlength + 2)\n",
    "            push!(tgt_sen, tgt_eos)\n",
    "        end\n",
    "        \n",
    "        #Source part. \n",
    "        src_eos = d.src.vocab.eos\n",
    "        eos_num = srclength - length(src_sentence)\n",
    "        i = 0\n",
    "        while i!= eos_num\n",
    "            push!(src_sen, src_eos)\n",
    "            i += 1\n",
    "        end\n",
    "        for w in src_sentence\n",
    "            push!(src_sen, w) \n",
    "        end        \n",
    "        push!(x, src_sen)\n",
    "        push!(y, tgt_sen)\n",
    "    end\n",
    "    #println(size(hcat(x...)))\n",
    "    #println(length(hcat(x...)))\n",
    "    #println(length(hcat(x...)[1]))\n",
    "    return Array(transpose(hcat(x...))), Array(transpose(hcat(y...)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing MTData\n",
      "└ @ Main In[8]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing MTData\"\n",
    "dtrn = MTData(tr_train, en_train)\n",
    "ddev = MTData(tr_dev, en_dev)\n",
    "dtst = MTData(tr_test, en_test)\n",
    "\n",
    "x,y = first(dtst)\n",
    "@test length(collect(dtst)) == 48\n",
    "@test size.((x,y)) == ((128,10),(128,24))\n",
    "@test x[1,1] == tr_vocab.eos\n",
    "@test x[1,end] != tr_vocab.eos\n",
    "@test y[1,1] == en_vocab.eos\n",
    "@test y[1,2] != en_vocab.eos\n",
    "@test y[1,end] == en_vocab.eos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Sequence to sequence model without attention\n",
    "\n",
    "In this part we will define a simple sequence to sequence encoder-decoder model for\n",
    "machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct S2S_v1\n",
    "    srcembed::Embed     # source language embedding\n",
    "    encoder::RNN        # encoder RNN (can be bidirectional)\n",
    "    tgtembed::Embed     # target language embedding\n",
    "    decoder::RNN        # decoder RNN\n",
    "    projection::Linear  # converts decoder output to vocab scores\n",
    "    dropout::Real       # dropout probability to prevent overfitting\n",
    "    srcvocab::Vocab     # source language vocabulary\n",
    "    tgtvocab::Vocab     # target language vocabulary\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 constructor\n",
    "\n",
    "Define the S2S_v1 constructor using your predefined layer types (Embed, Linear), and the\n",
    "Knet RNN type. Please review the RNN documentation using `@doc RNN`, paying attention to\n",
    "the following options in particular: `numLayers`, `bidirectional`, `dropout`, `dataType`,\n",
    "`usegpu`. The last two are important if you experiment with array types other than the\n",
    "default `KnetArray{Float32}`: make sure the RNNs use the same array type as the other\n",
    "layers. Note that if the encoder is bidirectional, its `numLayers` should be half of the\n",
    "decoder so that their hidden states match in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2S_v1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function S2S_v1(hidden::Int,         # hidden size for both the encoder and decoder RNN\n",
    "                srcembsz::Int,       # embedding size for source language\n",
    "                tgtembsz::Int,       # embedding size for target language\n",
    "                srcvocab::Vocab,     # vocabulary for source language\n",
    "                tgtvocab::Vocab;     # vocabulary for target language\n",
    "                layers=1,            # number of layers\n",
    "                bidirectional=false, # whether encoder RNN is bidirectional\n",
    "                dropout=0)           # dropout probability\n",
    "    # Your code here\n",
    "    src_embedd_layer = Embed(length(srcvocab.i2w), srcembsz)\n",
    "    tgt_embedd_layer = Embed(length(tgtvocab.i2w), tgtembsz)\n",
    "    proj = Linear(hidden, length(tgtvocab.i2w))\n",
    "    encoder_layers = layers\n",
    "    if bidirectional \n",
    "        encoder_layers /= 2\n",
    "    end\n",
    "    \n",
    "    encoder = RNN(srcembsz, hidden, numLayers = encoder_layers, bidirectional = bidirectional, dropout = dropout)\n",
    "    decoder = RNN(tgtembsz, hidden, numLayers = layers, dropout = dropout)\n",
    "    S2S_v1(src_embedd_layer, encoder, tgt_embedd_layer, decoder, proj, dropout, srcvocab, tgtvocab)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2S_v1 loss function\n",
    "\n",
    "Define the S2S_v1 loss function that takes `src`, a source language minibatch, and `tgt`,\n",
    "a target language minibatch and returns either a `(total_loss, num_words)` pair if\n",
    "`average=false`, or `(total_loss/num_words)` average if `average=true`.\n",
    "\n",
    "Assume that `src` and `tgt` are integer arrays of size `(B,Tx)` and `(B,Ty)` respectively,\n",
    "where `B` is the batch size, `Tx` is the length of the longest source sequence, `Ty` is\n",
    "the length of the longest target sequence. The `src` sequences only contain words, the\n",
    "`tgt` sequences surround the words with `eos` tokens at the start and end. This allows\n",
    "columns `tgt[:,1:end-1]` to be used as the decoder input and `tgt[:,2:end]` as the desired\n",
    "decoder output.\n",
    "\n",
    "Assume any shorter sentences in the batches have been padded with extra `eos` tokens on\n",
    "the left for `src` and on the right for `tgt`. Don't worry about masking `src` for the\n",
    "encoder, it doesn't have a significant effect on the loss. However do mask `tgt` before\n",
    "`nll`: you do not want the padding tokens to be counted in the loss calculation.\n",
    "\n",
    "Please review `@doc RNN`: in particular the `r.c` and `r.h` fields can be used to get/set\n",
    "the cell and hidden arrays of an RNN (note that `0` and `nothing` act as special values).\n",
    "\n",
    "RNNs take a dropout value at construction and apply dropout to the input of every layer if\n",
    "it is non-zero. You need to handle dropout for other layers in the loss function or in\n",
    "layer definitions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src, tgt; average=true)\n",
    "    s.encoder.c, s.encoder.h = 0, 0\n",
    "    src_embed_out = s.srcembed(src) #;@show typeof(src_embed_out),size(src_embed_out)\n",
    "    s.encoder(src_embed_out) #;@show s.encoder\n",
    "    s.decoder.h = s.encoder.h\n",
    "    s.decoder.c = s.encoder.c\n",
    "    tgt_embed_out = s.tgtembed(tgt[:, 1:end-1]) #; @show size(tgt_embed_out)\n",
    "    y_de = s.decoder(tgt_embed_out) #; @show size(y_de,1),size(y_de,2),size(y_de,3);\n",
    "    y_reshaped = reshape(y_de, (size(y_de,1), size(y_de,2)*size(y_de,3))) # ;@show size(y_reshaped)\n",
    "    scores = s.projection(y_reshaped) #;@show size(scores)\n",
    "    target=tgt[:, 2:end]\n",
    "    mask!(target, s.srcvocab.eos)#;    @show size(dec_op)\n",
    "    target=reshape(target,:)\n",
    "    nll(scores,target, average=average)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing S2S_v1\n",
      "└ @ Main In[12]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing S2S_v1\"\n",
    "Knet.seed!(1)\n",
    "model = S2S_v1(512, 512, 512, tr_vocab, en_vocab; layers=2, bidirectional=true, dropout=0.2)\n",
    "(x,y) = first(dtst)\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "@test all(model(x,y; average=false) .≈ (14097.471f0, 1432))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.844618f0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss for a whole dataset\n",
    "\n",
    "Define a `loss(model, data)` which returns a `(Σloss, Nloss)` pair if `average=false` and\n",
    "a `Σloss/Nloss` average if `average=true` for a whole dataset. Assume that `data` is an\n",
    "iterator of `(x,y)` pairs such as `MTData` and `model(x,y;average)` is a model like\n",
    "`S2S_v1` that computes loss on a single `(x,y)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(model, data; average=true)\n",
    "    loss,insts = 0,0\n",
    "    loss_sum,loss_total = 0,0\n",
    "    for (x, y) in data\n",
    "        loss, insts = model(x, y, average=false)\n",
    "        loss_total += insts\n",
    "        loss_sum += loss\n",
    "    end\n",
    "    average ? (return loss_sum/loss_total) : (return loss_sum, loss_total)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing loss\n",
      "└ @ Main In[17]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing loss\"\n",
    "@test all(loss(model, dtst, average=false) .≈ (1.0429117f6, 105937))\n",
    "# Your loss can be slightly different due to different ordering of words in the vocabulary.\n",
    "# The reference vocabulary starts with eos, unk, followed by words in decreasing frequency.\n",
    "# Also, because we do not mask src, different batch sizes may lead to slightly different\n",
    "# losses. The test above gives (1.0429178f6, 105937) with batchsize==1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SGD_v1\n",
    "\n",
    "The following function can be used to train our model. `trn` is the training data, `dev`\n",
    "is used to determine the best model, `tst...` can be zero or more small test datasets for\n",
    "loss reporting. It returns the model that does best on `dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(model, trn, dev, tst...)\n",
    "    bestmodel, bestloss = deepcopy(model), loss(model, dev)\n",
    "    progress!(adam(model, trn), steps=100) do y\n",
    "        losses = [ loss(model, d) for d in (dev,tst...) ]\n",
    "        if losses[1] < bestloss\n",
    "            bestmodel, bestloss = deepcopy(model), losses[1]\n",
    "        end\n",
    "        return (losses...,)\n",
    "    end\n",
    "    return bestmodel\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get under 3.40 dev loss with the following settings in 10\n",
    "epochs. The training speed on a V100 is about 3 mins/epoch or 40K words/sec, K80 is about\n",
    "6 times slower. Using settings closer to the Luong paper (per-sentence loss rather than\n",
    "per-word loss, SGD with lr=1, gclip=1 instead of Adam), you can get to 3.17 dev loss in\n",
    "about 25 epochs. Using dropout and shuffling batches before each epoch significantly\n",
    "improve the dev loss. You can play around with hyperparameters but I doubt results will\n",
    "get much better without attention. To verify your training, here is the dev loss I\n",
    "observed at the beginning of each epoch in one training session:\n",
    "`[9.83, 4.60, 3.98, 3.69, 3.52, 3.41, 3.35, 3.32, 3.30, 3.31, 3.33]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training S2S_v1\n",
      "└ @ Main In[18]:1\n",
      "┣████████████████████┫ [100.00%, 14330/14330, 36:32/36:32, 6.54i/s] (3.481387f0, 2.0574226f0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "S2S_v1(Embed(P(KnetArray{Float32,2}(512,38126))), LSTM(input=512,hidden=512,bidirectional,dropout=0.2), Embed(P(KnetArray{Float32,2}(512,18857))), LSTM(input=512,hidden=512,layers=2,dropout=0.2), Linear(P(KnetArray{Float32,2}(18857,512)), P(KnetArray{Float32,1}(18857))), 0.2, Vocab(Dict(\"dev\" => 1277,\"komuta\" => 13566,\"ellisi\" => 25239,\"adresini\" => 22820,\"yüzeyi\" => 4051,\"paris'te\" => 9494,\"kafamdaki\" => 18790,\"yüzeyinde\" => 5042,\"geçerlidir\" => 6612,\"kökten\" => 7774…), [\"<unk>\", \"<s>\", \".\", \",\", \"bir\", \"ve\", \"bu\", \"''\", \"``\", \"için\"  …  \"seçmemiz\", \"destekleyip\", \"karşılaştırılabilir\", \"ördeğin\", \"gününüzü\", \"bağışçı\", \"istismara\", \"yaşça\", \"tedci\", \"fakültesi'nde\"], 1, 2, split), Vocab(Dict(\"middle-income\" => 13398,\"photosynthesis\" => 7689,\"polarizing\" => 17881,\"henry\" => 4248,\"abducted\" => 15691,\"rises\" => 6225,\"hampshire\" => 13888,\"whiz\" => 16835,\"cost-benefit\" => 13137,\"progression\" => 5549…), [\"<unk>\", \"<s>\", \",\", \".\", \"the\", \"and\", \"to\", \"of\", \"a\", \"that\"  …  \"archaea\", \"handshake\", \"brit\", \"wiper\", \"heroines\", \"coca\", \"exceptionally\", \"gallbladder\", \"autopsies\", \"linguistics\"], 1, 2, split))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Training S2S_v1\"\n",
    "epochs = 10\n",
    "ctrn = collect(dtrn)\n",
    "trnx10 = collect(flatten(shuffle!(ctrn) for i in 1:epochs))\n",
    "trn20 = ctrn[1:20]\n",
    "dev38 = collect(ddev)\n",
    "#Uncomment this to train the model (This takes about 30 mins on a V100):\n",
    "model = train!(model, trnx10, dev38, trn20)\n",
    "#Uncomment this to save the model:\n",
    "#Knet.save(\"s2s_v2.jld2\",\"model\",model)\n",
    "#Uncomment this to load the model:\n",
    "#model = Knet.load(\"s2s_v2.jld2\",\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating translations\n",
    "\n",
    "With a single argument, a `S2S_v1` object should take it as a batch of source sentences\n",
    "and generate translations for them. After passing `src` through the encoder and copying\n",
    "its hidden states to the decoder, the decoder is run starting with an initial input of all\n",
    "`eos` tokens. Highest scoring tokens are appended to the output and used as input for the\n",
    "subsequent decoder steps.  The decoder should stop generating when all sequences in the\n",
    "batch have generated `eos` or when `stopfactor * size(src,2)` decoder steps are reached. A\n",
    "correctly shaped target language batch should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::S2S_v1)(src::Matrix{Int}; stopfactor = 3)\n",
    "    batch_size = size(src)[1]\n",
    "    src_length = size(src)[2]\n",
    "    s.encoder.h, s.encoder.c = 0, 0\n",
    "    s.encoder(s.srcembed(src))\n",
    "    s.decoder.h, s.decoder.c = s.encoder.h, s.encoder.c\n",
    "    dec_input = s.tgtembed(repeat([s.tgtvocab.eos], batch_size))\n",
    "    trans_sens = []\n",
    "    eos_cond = repeat([false], batch_size)\n",
    "    while !all(eos_cond) \n",
    "        hidden_state = s.decoder(dec_input)\n",
    "        scores = s.projection(hidden_state)\n",
    "        tokens = vec(map(x -> x[1], argmax(scores, dims=1)))\n",
    "        push!(trans_sens, tokens)\n",
    "        eos_cond[findall(i->i==s.tgtvocab.eos, tokens)] .= true\n",
    "        if length(trans_sens) >= stopfactor * size(src,2)\n",
    "            break\n",
    "        end\n",
    "        dec_input = s.tgtembed(tokens)\n",
    "    end\n",
    "    #println(\"\\n\\n\\n\")\n",
    "    return hcat(trans_sens...)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int2str (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utility to convert int arrays to sentence strings\n",
    "function int2str(y,vocab)\n",
    "    y = vec(y)\n",
    "    ysos = findnext(w->!isequal(w,vocab.eos), y, 1)\n",
    "    ysos == nothing && return \"\"\n",
    "    yeos = something(findnext(isequal(vocab.eos), y, ysos), 1+length(y))\n",
    "    join(vocab.i2w[y[ysos:yeos-1]], \" \")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Generating some translations\n",
      "└ @ Main In[21]:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC: çin'e 15 şubat 2006'da ulaştım .\n",
      "REF: i made it to china on february 15 , 2006 .\n",
      "OUT: and in the summer , i went to the moon , and i was in the middle .\n"
     ]
    }
   ],
   "source": [
    "@info \"Generating some translations\"\n",
    "d = MTData(tr_dev, en_dev, batchsize=1) |> collect\n",
    "(src,tgt) = rand(d)\n",
    "out = model(src)\n",
    "println(\"SRC: \", int2str(src,model.srcvocab))\n",
    "println(\"REF: \", int2str(tgt,model.tgtvocab))\n",
    "println(\"OUT: \", int2str(out,model.tgtvocab))\n",
    "# Here is a sample output:\n",
    "# SRC: çin'e 15 şubat 2006'da ulaştım .\n",
    "# REF: i made it to china on february 15 , 2006 .\n",
    "# OUT: i got to china , china , at the last 15 years ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating BLEU\n",
    "\n",
    "BLEU is the most commonly used metric to measure translation quality. The following should\n",
    "take a model and some data, generate translations and calculate BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bleu (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bleu(s2s,d::MTData)\n",
    "    d = MTData(d.src,d.tgt,batchsize=1)\n",
    "    reffile = d.tgt.file\n",
    "    hypfile,hyp = mktemp()\n",
    "    for (x,y) in progress(collect(d))\n",
    "        g = s2s(x)\n",
    "        for i in 1:size(y,1)\n",
    "            println(hyp, int2str(g[i,:], d.tgt.vocab))\n",
    "        end\n",
    "    end\n",
    "    close(hyp)\n",
    "    isfile(\"multi-bleu.perl\") || download(\"https://github.com/moses-smt/mosesdecoder/raw/master/scripts/generic/multi-bleu.perl\", \"multi-bleu.perl\")\n",
    "    run(pipeline(`cat $hypfile`,`perl multi-bleu.perl $reffile`))\n",
    "    return hypfile\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating dev BLEU takes about 45 secs on a V100. We get about 8.0 BLEU which is pretty\n",
    "low. As can be seen from the sample translations a loss of ~3+ (perplexity ~20+) or a BLEU\n",
    "of ~8 is not sufficient to generate meaningful translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Calculating BLEU\n",
      "└ @ Main In[23]:1\n",
      "┣████████████████████┫ [100.00%, 4045/4045, 01:02/01:02, 64.95i/s] \n",
      "perl: warning: Setting locale failed.\n",
      "perl: warning: Please check that your locale settings:\n",
      "\tLANGUAGE = (unset),\n",
      "\tLC_ALL = (unset),\n",
      "\tLC_CTYPE = \"UTF-8\",\n",
      "\tLANG = \"en_US.UTF-8\"\n",
      "    are supported and installed on your system.\n",
      "perl: warning: Falling back to the standard locale (\"C\").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 8.04, 37.4/11.9/4.7/2.0 (BP=1.000, ratio=1.072, hyp_len=88469, ref_len=82502)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/tmp/jl_evIopI\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Calculating BLEU\"\n",
    "bleu(model, ddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the quality of translations we can use more training data, different training\n",
    "and model parameters, or preprocess the input/output: e.g. splitting Turkish words to make\n",
    "suffixes look more like English function words may help. Other architectures,\n",
    "e.g. attention and transformer, perform significantly better than this simple S2S model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
